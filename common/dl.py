import os
import time
import hashlib
import asyncio
from typing import Optional, List
from aiohttp import ClientError, ClientSession
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, DownloadColumn
import aiofiles
from .log import log

class NotFoundError(Exception):
    """å½“èµ„æºä¸å­˜åœ¨æ—¶æŠ›å‡ºå¼‚å¸¸"""
    def __init__(self, url):
        super().__init__(f"Resource not found at {url}")
        self.url = url

class HttpError(Exception):
    """HTTPé”™è¯¯å¼‚å¸¸"""
    def __init__(self, status_code: int, url: str):
        super().__init__(f"HTTP Error {status_code} at {url}")
        self.status_code = status_code
        self.url = url

class Downloader:
    @staticmethod
    def get_cn_urls(repo: str, sha: str, path: str) -> List[str]:
        return [
            f'https://jsdelivr.pai233.top/gh/{repo}@{sha}/{path}',
            f'https://cdn.jsdmirror.com/gh/{repo}@{sha}/{path}',
            f'https://raw.gitmirror.com/{repo}/{sha}/{path}',
            f'https://raw.dgithub.xyz/{repo}/{sha}/{path}',
            f'https://gh.akass.cn/{repo}/{sha}/{path}'
        ]

    @staticmethod
    def get_default_url(repo: str, sha: str, path: str) -> str:
        return f'https://raw.githubusercontent.com/{repo}/{sha}/{path}'

    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir
        self.memory_cache = {}
        if cache_dir:
            os.makedirs(cache_dir, exist_ok=True)

    def _get_cache_path(self, cache_key: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        filename = hashlib.md5(cache_key.encode()).hexdigest()
        return os.path.join(self.cache_dir, filename) if self.cache_dir else ""

    async def _try_read_cache(self, cache_key: str) -> Optional[bytearray]:
        """å°è¯•ä»ç¼“å­˜è¯»å–å†…å®¹"""
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]

        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        if self.cache_dir:
            cache_path = self._get_cache_path(cache_key)
            try:
                async with aiofiles.open(cache_path, 'rb') as f:
                    content = bytearray(await f.read())
                    self.memory_cache[cache_key] = content
                    return content
            except FileNotFoundError:
                pass
            except Exception as e:
                log.error(f"ç¼“å­˜è¯»å–å¤±è´¥: {str(e)}")
        return None

    async def _write_cache(self, cache_key: str, content: bytearray):
        """å†™å…¥ç¼“å­˜"""
        self.memory_cache[cache_key] = content
        if self.cache_dir:
            cache_path = self._get_cache_path(cache_key)
            try:
                async with aiofiles.open(cache_path, 'wb') as f:
                    await f.write(content)
            except Exception as e:
                log.error(f"ç¼“å­˜å†™å…¥å¤±è´¥: {str(e)}")

    async def _download_url(self, url: str, session: ClientSession, path: str, 
                           chunk_size: int, progress: Progress, task: int) -> bytearray:
        """ä»å•ä¸ªURLä¸‹è½½å†…å®¹"""
        async with session.get(url, ssl=False, timeout=30) as response:
            if response.status == 200:
                content = bytearray()
                async for chunk in response.content.iter_chunked(chunk_size):
                    content.extend(chunk)
                    progress.update(task, advance=len(chunk))
                return content
            
            if response.status == 404:
                raise NotFoundError(url)
            raise HttpError(response.status, url)

    async def get(self, sha: str, path: str, repo: str, session: ClientSession,
                 chunk_size: int = 1024, timeout: int = 30) -> bytearray:
        cache_key = f"{repo}@{sha}/{path}"
        
        # å°è¯•è¯»å–ç¼“å­˜
        cached = await self._try_read_cache(cache_key)
        if cached is not None:
            return cached

        # ç”ŸæˆURLåˆ—è¡¨
        url_list = (
            self.get_cn_urls(repo, sha, path) 
            if os.environ.get('IS_CN') == 'yes' 
            else [self.get_default_url(repo, sha, path)]
        )

        # åˆå§‹åŒ–è¿›åº¦æ¡
        with Progress(
            TextColumn("[progress.description]{task.description}", style="#66CCFF"),
            BarColumn(style="#66CCFF", complete_style="#4CE49F", finished_style="#2FE9D9"),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%", style="#66CCFF"),
            TimeElapsedColumn(),
            DownloadColumn(),
        ) as progress:
            task_id = progress.add_task(f"ğŸ“¥ ä¸‹è½½ {path} ä¸­...", total=0)

            retry_count = 3
            errors = []
            remaining_urls = url_list.copy()

            while retry_count > 0 and remaining_urls:
                current_errors = []
                for url in list(remaining_urls):
                    try:
                        # æ›´æ–°è¿›åº¦æ¡æè¿°ä¸ºå½“å‰å°è¯•çš„URL
                        progress.update(task_id, description=f"ğŸ“¥ å°è¯• {url}")

                        # è·å–å†…å®¹å¹¶æ›´æ–°è¿›åº¦æ¡
                        content = await self._download_url(
                            url, session, path, chunk_size, progress, task_id
                        )
                        
                        # å†™å…¥ç¼“å­˜å¹¶è¿”å›å†…å®¹
                        await self._write_cache(cache_key, content)
                        return content

                    except NotFoundError as e:
                        log.warning(f"âš ï¸ èµ„æºä¸å­˜åœ¨: {e.url}")
                        remaining_urls.remove(url)
                    except (ClientError, HttpError, asyncio.TimeoutError) as e:
                        error_msg = f"{type(e).__name__}: {str(e)}"
                        current_errors.append(f"URL {url}: {error_msg}")
                        log.error(f"âŒ ä¸‹è½½å¤±è´¥: {path} @ {url} - {error_msg}")

                # è®°å½•æœ¬è½®é”™è¯¯å¹¶å‡†å¤‡é‡è¯•
                if current_errors:
                    errors.extend(current_errors)
                    retry_count -= 1
                    if retry_count > 0 and remaining_urls:
                        log.warning(f"âš ï¸ å‰©ä½™é‡è¯•æ¬¡æ•°: {retry_count} - {path}")
                        await asyncio.sleep(1)  # é‡è¯•é—´éš”

        # æ‰€æœ‰å°è¯•å¤±è´¥åæŠ›å‡ºå¼‚å¸¸
        error_log = "\n".join(errors)
        log.error(f"âŒ æ— æ³•ä¸‹è½½ {path}ï¼Œæ‰€æœ‰å°è¯•å¤±è´¥:\n{error_log}")
        raise Exception(f"æ— æ³•ä¸‹è½½ {path}ï¼Œè¯¦ç»†é”™è¯¯:\n{error_log}")